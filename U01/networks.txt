Neural Network Configurations:
net: 1, [Test network: only 5 epochs], t: 15.547565937042236, optimization algorithm: sgd; learning rate: 0.1; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: linear, ; 
net: 2, [Original network], t: 55.367087841033936, optimization algorithm: sgd; learning rate: 0.1; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 3, [Original Network with 50 epochs], t: 134.27706289291382, optimization algorithm: sgd; learning rate: 0.1; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 4, [Original Network with stddev set to 1/(2 * sqrt(units)], t: 55.337356090545654, optimization algorithm: sgd; learning rate: 0.1; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 5, [Original Network with learning rate of 0.15], t: 55.536888122558594, optimization algorithm: sgd; learning rate: 0.15; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 6, [Original Network with learning rate of 0.3], t: 55.000417947769165, optimization algorithm: sgd; learning rate: 0.3; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 7, [Original Network with more layers and different activation functions], t: 92.4527440071106, optimization algorithm: sgd; learning rate: 0.15; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 200, activation function: sigmoid, ; Layer 3: neurons: 100, activation function: tanh, ; Layer 4: neurons: 10, activation function: linear, ; Layer 5: neurons: 10, activation function: softmax, ; 
net: 8, [Original network with batch size of 100], t: 50.7977659702301, optimization algorithm: sgd; learning rate: 0.1; batch size: 100; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 9, [NN5 with 40 epochs], t: 109.95614194869995, optimization algorithm: sgd; learning rate: 0.3; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 10, [NN5 with 60 epochs], t: 164.02783513069153, optimization algorithm: sgd; learning rate: 0.3; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 11, [NN10 with batch size 100], t: 148.44879484176636, optimization algorithm: sgd; learning rate: 0.3; batch size: 100; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 12, [NN10 with batch size 100], t: 195.75761198997498, optimization algorithm: sgd; learning rate: 0.3; batch size: 100; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 13, [Original Network with learning rate of 0.45], t: 73.66511011123657, learning rate: 0.45; optimization algorithm: sgd; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 14, [Original Network with learning rate of 0.60], t: 69.53807592391968, learning rate: 0.6; optimization algorithm: sgd; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 15, [Original Network with learning rate of 0.30 and batch size of 150], t: 61.82280087471008, learning rate: 0.3; optimization algorithm: sgd; batch size: 150; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 16, [Original Network with learning rate of 0.30 and batch size of 300], t: 58.47657012939453, learning rate: 0.3; optimization algorithm: sgd; batch size: 300; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 17, [Original Network with learning rate of 0.30 and batch size of 450], t: 54.083858013153076, learning rate: 0.3; optimization algorithm: sgd; batch size: 450; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 18, [Original Network with learning rate of 0.30 and batch size of 600], t: 53.75694489479065, learning rate: 0.3; optimization algorithm: sgd; batch size: 600; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 19, [Original Network with learning rate of 0.30 and batch size of 600 and 40 epochs], t: 107.23840308189392, learning rate: 0.3; optimization algorithm: sgd; batch size: 600; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 20, [Original Network with learning rate of 0.30 and batch size of 600 and 60 epochs], t: 153.09728002548218, learning rate: 0.3; optimization algorithm: sgd; batch size: 600; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 21, [Original Network with learning rate of 0.30 and batch size of 600 and 80 epochs], t: 188.6409411430359, learning rate: 0.3; optimization algorithm: sgd; batch size: 600; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 22, [Original Network with learning rate of 0.30 and batch size of 600 and 100 epochs], t: 219.08402395248413, learning rate: 0.3; optimization algorithm: sgd; batch size: 600; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 23, [Original Network with learning rate of 0.30 and batch size of 600 and 60 epochs and 4 layers (rrr_)], t: 156.9374978542328, learning rate: 0.3; optimization algorithm: sgd; batch size: 600; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 100, activation function: relu, ; Layer 4: neurons: 10, activation function: linear, ; Layer 5: neurons: 10, activation function: softmax, ; 
net: 24, [Original Network with learning rate of 0.30 and batch size of 600 and 60 epochs and 4 layers (sss_)], t: 196.6582429409027, learning rate: 0.3; optimization algorithm: sgd; batch size: 600; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: sigmoid, ; Layer 2: neurons: 100, activation function: sigmoid, ; Layer 3: neurons: 100, activation function: sigmoid, ; Layer 4: neurons: 10, activation function: linear, ; Layer 5: neurons: 10, activation function: softmax, ; 
net: 25, [Original Network with learning rate of 0.30 and batch size of 600 and 60 epochs and 4 layers (ttt_)], t: 205.41926407814026, learning rate: 0.3; optimization algorithm: sgd; batch size: 600; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: tanh, ; Layer 2: neurons: 100, activation function: tanh, ; Layer 3: neurons: 100, activation function: tanh, ; Layer 4: neurons: 10, activation function: linear, ; Layer 5: neurons: 10, activation function: softmax, ; 
net: 26, [Original Network with learning rate of 0.30 and batch size of 600 and 60 epochs and 4 layers (rst_)], t: 197.43714594841003, learning rate: 0.3; optimization algorithm: sgd; batch size: 600; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: sigmoid, ; Layer 3: neurons: 100, activation function: tanh, ; Layer 4: neurons: 10, activation function: linear, ; Layer 5: neurons: 10, activation function: softmax, ; 
net: 27, [Original Network with learning rate of 0.30 and batch size of 64 and 40 epochs and lr decay of 0.95], t: 104.49712800979614, optimization algorithm: sgd; learning rate: 0.03662596462105439; batch size: 64; Layer 0: activation function: linear, neurons: 784, ; Layer 1: activation function: relu, neurons: 100, ; Layer 2: activation function: relu, neurons: 100, ; Layer 3: activation function: linear, neurons: 10, ; Layer 4: activation function: softmax, neurons: 10, ; 
net: 28, [Original Network with learning rate of 0.30 and batch size of 64 and 40 epochs and lr decay of 0.90], t: 106.62795996665955, optimization algorithm: sgd; learning rate: 0.003990838394187343; batch size: 64; Layer 0: activation function: linear, neurons: 784, ; Layer 1: activation function: relu, neurons: 100, ; Layer 2: activation function: relu, neurons: 100, ; Layer 3: activation function: linear, neurons: 10, ; Layer 4: activation function: softmax, neurons: 10, ; 
net: 29, [Original Network with learning rate of 0.30 and batch size of 64 and 40 epochs and lr decay of 0.85], t: 107.1847472190857, optimization algorithm: sgd; learning rate: 0.00038308681872231557; batch size: 64; Layer 0: activation function: linear, neurons: 784, ; Layer 1: activation function: relu, neurons: 100, ; Layer 2: activation function: relu, neurons: 100, ; Layer 3: activation function: linear, neurons: 10, ; Layer 4: activation function: softmax, neurons: 10, ; 
net: 30, [Original Network with learning rate of 0.30 and batch size of 64 and 40 epochs and lr decay of 0.80], t: 105.53222894668579, optimization algorithm: sgd; learning rate: 3.1901471898838065e-05; batch size: 64; Layer 0: activation function: linear, neurons: 784, ; Layer 1: activation function: relu, neurons: 100, ; Layer 2: activation function: relu, neurons: 100, ; Layer 3: activation function: linear, neurons: 10, ; Layer 4: activation function: softmax, neurons: 10, ; 
net: 31, [Original Network with learning rate of 0.30 and batch size of 64 and 60 epochs and lr decay of 0.99], t: 159.5382878780365, optimization algorithm: sgd; learning rate: 0.16250552279005603; batch size: 64; Layer 0: activation function: linear, neurons: 784, ; Layer 1: activation function: relu, neurons: 100, ; Layer 2: activation function: relu, neurons: 100, ; Layer 3: activation function: linear, neurons: 10, ; Layer 4: activation function: softmax, neurons: 10, ; 
net: 32, [Original Network with learning rate of 0.30 and batch size of 64 and 60 epochs and lr decay of 0.98], t: 157.1243281364441, optimization algorithm: sgd; learning rate: 0.08748062395148343; batch size: 64; Layer 0: activation function: linear, neurons: 784, ; Layer 1: activation function: relu, neurons: 100, ; Layer 2: activation function: relu, neurons: 100, ; Layer 3: activation function: linear, neurons: 10, ; Layer 4: activation function: softmax, neurons: 10, ; 
net: 33, [Original Network with learning rate of 0.30 and batch size of 64 and 60 epochs and lr decay of 0.97], t: 158.60009908676147, optimization algorithm: sgd; learning rate: 0.046794740685278745; batch size: 64; Layer 0: activation function: linear, neurons: 784, ; Layer 1: activation function: relu, neurons: 100, ; Layer 2: activation function: relu, neurons: 100, ; Layer 3: activation function: linear, neurons: 10, ; Layer 4: activation function: softmax, neurons: 10, ; 
net: 34, [Original Network with learning rate of 0.30 and batch size of 64 and 60 epochs and lr decay of 0.96], t: 159.36705613136292, optimization algorithm: sgd; learning rate: 0.02486946657171013; batch size: 64; Layer 0: activation function: linear, neurons: 784, ; Layer 1: activation function: relu, neurons: 100, ; Layer 2: activation function: relu, neurons: 100, ; Layer 3: activation function: linear, neurons: 10, ; Layer 4: activation function: softmax, neurons: 10, ; 

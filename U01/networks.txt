Neural Network Configurations:
net: 1, [Test network: only 5 epochs], t: 15.547565937042236, optimization algorithm: sgd; learning rate: 0.1; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: linear, ; 
net: 2, [Original network], t: 55.367087841033936, optimization algorithm: sgd; learning rate: 0.1; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 3, [Original Network with 50 epochs], t: 134.27706289291382, optimization algorithm: sgd; learning rate: 0.1; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 4, [Original Network with stddev set to 1/(2 * sqrt(units)], t: 55.337356090545654, optimization algorithm: sgd; learning rate: 0.1; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 5, [Original Network with learning rate of 0.15], t: 55.536888122558594, optimization algorithm: sgd; learning rate: 0.15; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 6, [Original Network with learning rate of 0.3], t: 55.000417947769165, optimization algorithm: sgd; learning rate: 0.3; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 7, [Original Network with more layers and different activation functions], t: 92.4527440071106, optimization algorithm: sgd; learning rate: 0.15; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 200, activation function: sigmoid, ; Layer 3: neurons: 100, activation function: tanh, ; Layer 4: neurons: 10, activation function: linear, ; Layer 5: neurons: 10, activation function: softmax, ; 
net: 8, [Original network with batch size of 100], t: 50.7977659702301, optimization algorithm: sgd; learning rate: 0.1; batch size: 100; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 9, [NN5 with 40 epochs], t: 109.95614194869995, optimization algorithm: sgd; learning rate: 0.3; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 10, [NN5 with 60 epochs], t: 164.02783513069153, optimization algorithm: sgd; learning rate: 0.3; batch size: 64; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 11, [NN10 with batch size 100], t: 148.44879484176636, optimization algorithm: sgd; learning rate: 0.3; batch size: 100; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
net: 12, [NN10 with batch size 100], t: 195.75761198997498, optimization algorithm: sgd; learning rate: 0.3; batch size: 100; Layer 0: neurons: 784, activation function: linear, ; Layer 1: neurons: 100, activation function: relu, ; Layer 2: neurons: 100, activation function: relu, ; Layer 3: neurons: 10, activation function: linear, ; Layer 4: neurons: 10, activation function: softmax, ; 
